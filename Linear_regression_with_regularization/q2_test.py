# -*- coding: utf-8 -*-
"""Q2_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R-h6mR7sy_h7Yi1dKE4xCb8nKprgkVDj
"""

# Correction for Assignment 4 : In Q2, you have to implement SGD with momentum 
# from scratch (for fit_SGD_with_momentum) and not use any sklearn function
# call(as mentioned in comments in the template). 
# Apologies for the mixup.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from linearRegression.linear_regression import LinearRegression
from metrics import *
import time

np.random.seed(45)

###################################################
'''Data Generation'''''
N = 90
P = 10
X = pd.DataFrame(np.random.randn(N, P))
y = pd.Series(np.random.randn(N))
print(X.shape, y.shape)


reg = LinearRegression(fit_intercept=True)

###################################################
# Using SKLEARN's Linear Regression
print('SKLEARNS Linear Regression : ')
batchsize=X.shape[0]
t = time.time()
reg.fit_sklearn_LR(X,y)
print('Time taken: ', time.time()-t)
y_hat = reg.predict(X)
print(' Batch size=',batchsize,', RMSE: ', rmse(y_hat, y))
print(' Batch size=',batchsize,', MAE: ', mae(y_hat, y))
  
print("---------------------------")


###################################################
# fit_gradient_descent using the manually computed gradients for each of unregularized mse_loss, and mse_loss with ridge regularization
print('Batch Gradient Descent with manual gradient computation for unregularized objective : ')
batchsize=X.shape[0]
t = time.time()
reg.fit_gradient_descent(X,y, batch_size=batchsize, gradient_type='manual', penalty_type=None, alpha=0.01, num_iters=100, lr=0.001)
print('Time taken: ', time.time()-t)
y_hat = reg.predict(X)
print(' Batch size=',batchsize,', RMSE: ', rmse(y_hat, y))
print(' Batch size=',batchsize,', MAE: ', mae(y_hat, y))
  
print("---------------------------")

print('Batch Gradient Descent with manual gradient computation for ridge regression : ')
batchsize=X.shape[0]
t = time.time()
reg.fit_gradient_descent(X,y, batch_size=batchsize, gradient_type='manual', penalty_type='l2', alpha=0.01, num_iters=100, lr=0.001)
print('Time taken: ', time.time()-t)
y_hat = reg.predict(X)
print(' Batch size=',batchsize,', RMSE: ', rmse(y_hat, y))
print(' Batch size=',batchsize,', MAE: ', mae(y_hat, y))
  
print("---------------------------")

###################################################
# fit_gradient_descent using the JAX gradients for each of unregularized mse_loss, mse_loss with LASSO regularization and mse_loss with ridge regression
print('Batch Gradient Descent with JAX gradient computation for unregularized objective : ')
batchsize=X.shape[0]
t = time.time()
reg.fit_gradient_descent(X,y, batch_size=batchsize, gradient_type='jax', penalty_type=None, alpha=0.01, num_iters=100, lr=0.001)
print('Time taken: ', time.time()-t)

y_hat = reg.predict(X)
print(' Batch size=',batchsize,', RMSE: ', rmse(y_hat, y))
print(' Batch size=',batchsize,', MAE: ', mae(y_hat, y))
  
print("---------------------------")

print('Batch Gradient Descent with JAX gradient computation for lasso regression : ')
batchsize=X.shape[0]
t = time.time()
reg.fit_gradient_descent(X,y, batch_size=batchsize, gradient_type='jax', penalty_type='l1', alpha=0.01, num_iters=100, lr=0.001)
print('Time taken: ', time.time()-t)
y_hat = reg.predict(X)
print(' Batch size=',batchsize,', RMSE: ', rmse(y_hat, y))
print(' Batch size=',batchsize,', MAE: ', mae(y_hat, y))
  
print("---------------------------")

print('Batch Gradient Descent with JAX gradient computation for ridge regression : ')
batchsize=X.shape[0]
t = time.time()
reg.fit_gradient_descent(X,y, batch_size=batchsize, gradient_type='jax', penalty_type='l2', alpha=0.01, num_iters=100, lr=0.001)
print('Time taken: ', time.time()-t)
y_hat = reg.predict(X)
print(' Batch size=',batchsize,', RMSE: ', rmse(y_hat, y))
print(' Batch size=',batchsize,', MAE: ', mae(y_hat, y))
  
print("---------------------------")


###################################################
# fit_gradient_descent for running SGD on mse_loss with ridge regularization
print('Stochastic Gradient Descent with manual gradient computation for ridge regression : ')
batchsize=1
t = time.time()
reg.fit_gradient_descent(X,y, batch_size=batchsize, gradient_type='manual', penalty_type='l2', alpha=0.01, num_iters=100, lr=0.001)
print('Time taken: ', time.time()-t )
y_hat = reg.predict(X)
print(' Batch size=',batchsize,', RMSE: ', rmse(y_hat, y))
print(' Batch size=',batchsize,', MAE: ', mae(y_hat, y))
  
print("---------------------------")

print('Stochastic Gradient Descent with JAX gradient computation for ridge regression : ')
batchsize=X.shape[0]
t = time.time()
reg.fit_gradient_descent(X,y, batch_size=batchsize, gradient_type='jax', penalty_type='l2', alpha=0.01, num_iters=100, lr=0.001)
print('Time taken: ', time.time()-t)
y_hat = reg.predict(X)
print(' Batch size=',batchsize,', RMSE: ', rmse(y_hat, y))
print(' Batch size=',batchsize,', MAE: ', mae(y_hat, y))
  
print("---------------------------")
###################################################
# fit_gradient_descent for running minibatch SGD on mse_loss with ridge regularization
print('mini-batch SGD with manual gradient computation for ridge regression : ')
batchsize=16
t = time.time()
reg.fit_gradient_descent(X,y, batch_size=batchsize, gradient_type='manual', penalty_type='l2', alpha=0.01, num_iters=100, lr=0.001)
print('Time taken: ', time.time()-t)

y_hat = reg.predict(X)
print(' Batch size=',batchsize,', RMSE: ', rmse(y_hat, y))
print(' Batch size=',batchsize,', MAE: ', mae(y_hat, y))
  
print("---------------------------")

###################################################
# fit_SGD_with_momentum for running SGD on mse_loss with ridge regularization
print('SGD_with_momentum with manual gradient computation for rifge regression : ')
reg = LinearRegression(fit_intercept=True)
t = time.time() 
reg.fit_SGD_with_momentum(X,y, penalty='l2', alpha=0.01, num_iters = 100, lr=0.001, beta=0.9)
print('Time taken: ', time.time()-t)
y_hat = reg.predict(X)

print('RMSE: ', rmse(y_hat, y))
print('MAE: ', mae(y_hat, y))

print("---------------------------")

print('SGD_with_momentum  with JAX gradient computation for ridge regression : ')
reg = LinearRegression(fit_intercept=True)
t = time.time()
reg.fit_SGD_with_momentum(X,y, penalty='l2', alpha=0.01, num_iters = 100, lr=0.001, beta=0.9)
print('Time taken: ', time.time()-t)
y_hat = reg.predict(X)

print('RMSE: ', rmse(y_hat, y))
print('MAE: ', mae(y_hat, y))
###################################################


#TODO :  Call the different variants of gradient descent here (as given in Q2)

print("---------------------------")
print("Q3: Comparing different variants of gradient descent : Hyper parameter tuning")

num_iters = 40

lr = [0.001, 0.01, 0.1]
penalty_value = [0.001, 0.01, 0.1, 0.5]
beta = [0.01, 0.05, 0.1, 0.5, 0.9]


error_mat = []
time_mat = []
error_time_matrix = []
for i in range(len(lr)):
    for j in  range(len(penalty_value)):
        for k in range(len(beta)):
            start = time.time()
            LR = LinearRegression(fit_intercept=True)
            LR.fit_SGD_with_momentum(X, y, penalty='l2',alpha=penalty_value[j], num_iters=num_iters,beta=beta[k], lr=lr[i])
            y_hat = LR.predict(X)
            end = time.time()

            error_time_matrix.append([ lr[i], penalty_value[j], beta[k], rmse(y_hat, y), (end-start)])

df_error_time_matrix = pd.DataFrame(error_time_matrix, columns=['lr', 'penalty_value', 'beta', 'rmse', 'time'])
print("Error and time matrix for SGD with momentum")
print(df_error_time_matrix)

# df_error_time_matrix = pd.DataFrame(error_time_matrix, columns=['lr', 'penalty_value', 'beta', 'rmse', 'time'])

# start = time.time()
# LR = LinearRegression(fit_intercept=True)
# LR.fit_SGD_with_momentum(X, y, penalty='l2',alpha=0.1, num_iters=num_iters,beta=0.9, lr=0.1)
# y_hat = LR.predict(X)
# end = time.time()
# # print('index ',i,j,k)
# # print('lr, penalty_value, beta ', lr[i], penalty_value[j], beta[k])
# print('RMSE: ', rmse(y_hat, y))
# print('MAE: ', mae(y_hat, y))
# print('time taken: ', (end-start))

print('Best RMSE: ', df_error_time_matrix['rmse'].min())

print(df_error_time_matrix[df_error_time_matrix['rmse'] == df_error_time_matrix['rmse'].min()])